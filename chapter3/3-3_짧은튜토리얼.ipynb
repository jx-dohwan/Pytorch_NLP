{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.Tensor([[1,2], [3,4]])\n",
    "x = torch.from_numpy(np.array([[1,2],[3,4]]))\n",
    "x = np.array([[1,2],[3,4]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "- 파이토치는 자동으로 미분 및 역전파를 수행하는 autograd 기능을 가진다.\n",
    "- 따라서 대부분 텐서 간 연산들을 크게 신경 쓸 필요 없이 역전파 알고리즘을 수행하는 명령어를 호출하기만 하면 된다.\n",
    "- 파이토치는 텐서들 간에 연산을 수애할 때마다 동적으로 연산 그래프를 생성하여 연산의 결과물이 어떤 텐서로부터 어떤 연산을 통해서 왔는지 추적한다.\n",
    "- 따라서 우리가 최종적으로 나온 스칼라에 역전파 알고리즘을 통해 미분을 수행하도록 했을 때, 각 텐서는 자기 자식 노드에 해당하는 텐서와 연산을 자동으로 찾아 계속해서 역전파 알고리즘을 수행할 수 있도록 돕는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0573e-05, 2.0773e+20],\n",
      "        [1.9990e+20, 1.2914e-11]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.FloatTensor(2,2)\n",
    "y = torch.FloatTensor(2,2)\n",
    "y.requires_grad_(True)\n",
    "\n",
    "z = (x+y) + torch.FloatTensor(2,2)\n",
    "print(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x와 y를 생성하고 둘을 더하는 연산을 수행하면 x+y에 해당하는 텐서가 생성되어 연산 그래프에 할당된다.\n",
    "- 그리고 다시 생성된 텐서를 더해준 뒤, 이를 z에 할당한다.\n",
    "- 따라서 z로부터 역전파를 수행하게 되면 이미 생성된 연산 그래프를 따라서 미분 값을 전달할 수 있다.\n",
    "- 이것이 바로 케라스, 텐서플로와 다른 점이다.\n",
    "    - 케라스, 텐서플로우는 미리 정의한 연산들을 컴파일을 통해 고정한 후, 정해진 입력에 맞춰 텐서를 피드포워드해야한다.\n",
    "    - 반면 파이토치에는 정해진 연산이라는 것은 없고 모델은 배워야 하는 파라미터 텐서만 미리 알 고 있을 ㅃㄴ\n",
    "    - 그 가중치 파라미터들이 어떠한 연산을 통해 학습 또는 연산에 관여하는지 알 수 없다.\n",
    "    - 연산이 수행된 직후에 알 수 있을 뿐이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기울기를 구할 필요가 없는 연산의 경우는 with 문법을 사용하여 연산을 수행할 수 있다.\n",
    "- 역전파 알고리즘 수행이 필요없는 비 학습과정, 즉 예측, 추론등을 수행할 때 유용하며, 기울기를 구하기 위한(연산 그래프 생성등의) 사전 작업을 생략할 수 있으므로 연산 속도 및 메모리 사용 측면에서도 큰 이점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor(2,2)\n",
    "y = torch.FloatTensor(2,2)\n",
    "y.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = (x+y) + torch.FloatTensor(2,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드 포워드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44],\n",
      "        [7.0065e-44, 8.1275e-44, 7.1466e-44, 7.4269e-44, 8.1275e-44]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def linear(x, W, b):\n",
    "    y = torch.mm(x, W) + b\n",
    "\n",
    "    return y\n",
    "\n",
    "x = torch.FloatTensor(16,10)\n",
    "W = torch.FloatTensor(10,5)\n",
    "b = torch.FloatTensor(5)\n",
    "\n",
    "y = linear(x,W,b)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "- nn.Module 상속 객체 안에 nn.Module 상속 객체를 선언하여 변수로 사용할 수 있다.\n",
    "- 그리고 nn.Module의 forward()함수를 오버라이드하여 피드포워드를 구현할 수 있다.\n",
    "- 이외에도 nn.Module의 특징을 이용하여 한 번에 신경망 가중치 파라미터들을 저장 및 불러오기를 수행할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.FloatTensor(input_size, output_size)\n",
    "        self.b = torch.FloatTensor(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.mm(x, self.W) + self.b\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16,10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.7058e+22, 4.9596e-04, 2.1266e-02, 3.1892e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 5.2922e-04, 2.1231e-02, 3.1839e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 4.9522e-04, 2.1232e-02, 3.1841e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 4.9594e-04, 2.1230e-02, 3.1839e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 4.9594e-04, 2.1230e-02, 3.1837e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 3.5257e-02, 2.2240e-02, 3.3386e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 2.9852e-02, 7.2813e-02, 1.0925e-01, 4.1538e+21],\n",
      "        [6.7058e+22, 1.5734e-03, 6.7394e-02, 1.0089e-01, 4.1538e+21],\n",
      "        [6.7058e+22, 1.6961e-03, 9.8965e-02, 1.0127e-01, 4.1538e+21],\n",
      "        [6.7058e+22, 1.6984e-03, 6.7600e-02, 1.0126e-01, 4.1538e+21],\n",
      "        [6.7058e+22, 3.5341e-02, 4.6814e-02, 6.9517e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 1.5724e-03, 1.2466e+00, 1.9083e+00, 4.1538e+21],\n",
      "        [6.7058e+22, 1.0790e-03, 1.4859e+00, 2.2754e+00, 4.1538e+21],\n",
      "        [6.7058e+22, 1.6991e-03, 2.2306e-02, 3.1952e-02, 4.1538e+21],\n",
      "        [6.7058e+22, 1.5755e-03, 1.4849e+00, 2.2739e+00, 4.1538e+21],\n",
      "        [6.7058e+22, 1.5661e-03, 6.7396e-02, 1.0107e-01, 4.1538e+21]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7058e+22, 4.9596e-04, 2.1266e-02, 3.1892e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 5.2922e-04, 2.1231e-02, 3.1839e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 4.9522e-04, 2.1232e-02, 3.1841e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 4.9594e-04, 2.1230e-02, 3.1839e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 4.9594e-04, 2.1230e-02, 3.1837e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 3.5257e-02, 2.2240e-02, 3.3386e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 2.9852e-02, 7.2813e-02, 1.0925e-01, 4.1538e+21],\n",
       "        [6.7058e+22, 1.5734e-03, 6.7394e-02, 1.0089e-01, 4.1538e+21],\n",
       "        [6.7058e+22, 1.6961e-03, 9.8965e-02, 1.0127e-01, 4.1538e+21],\n",
       "        [6.7058e+22, 1.6984e-03, 6.7600e-02, 1.0126e-01, 4.1538e+21],\n",
       "        [6.7058e+22, 3.5341e-02, 4.6814e-02, 6.9517e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 1.5724e-03, 1.2466e+00, 1.9083e+00, 4.1538e+21],\n",
       "        [6.7058e+22, 1.0790e-03, 1.4859e+00, 2.2754e+00, 4.1538e+21],\n",
       "        [6.7058e+22, 1.6991e-03, 2.2306e-02, 3.1952e-02, 4.1538e+21],\n",
       "        [6.7058e+22, 1.5755e-03, 1.4849e+00, 2.2739e+00, 4.1538e+21],\n",
       "        [6.7058e+22, 1.5661e-03, 6.7396e-02, 1.0107e-01, 4.1538e+21]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "params = [p.size() for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameter라는 클래스를 사용하여 텐서를 감싸주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_size, output_size), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_size), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.mm(x, self.W) + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16,10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([10, 5]), torch.Size([5])]\n"
     ]
    }
   ],
   "source": [
    "params = [p.size() for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.FloatTensor(16,10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLinear(\n",
      "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(linear)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파 수행\n",
    "- 피드포워드를 통해 얻은 값에서 실제 정답값과의 차이를 계산하여 오류(손실)를 뒤로 전달하는 역전파 알고리즘을 수행해보자\n",
    "- 예를 들어 우리가 원하는 값이 100이라고 했을 때\n",
    "- linear의 결괏값 텐서의 합과 목푯값과의 거리를 구하고\n",
    "- 그 값에 대해 backward() 함수를 사용하여 기울기를 구한다.\n",
    "- 이때 에러(손실)값은 스칼라로 표현되어야 한다.\n",
    "- 벡터나 형렬의 형태여서는 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 100\n",
    "\n",
    "x = torch.FloatTensor(16,10)\n",
    "linear = MyLinear(10,5)\n",
    "y = linear(x)\n",
    "loss = (objective - y.sum())\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train()과 eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLinear(\n",
       "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.eval()\n",
    "linear.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train(), eval() 함수를 활용하면 사용자는 필요에 따라 모델에 대해 훈련 시와 추론시의 모드를 쉽게 전환할 수 있다.\n",
    "- nn.Module을 상속받아 구현하고 생성한 객체는 기본적으로 훈련모드이다.\n",
    "- 이를 eval()을 사용하여 추론 모드로 바꾸어주면, 드롭아웃 또는 배치 정규화와 같은 학습과 추론 시 서로 다른 forward()동작들을 하는 모듈들에 대해서도 각 상황에 따라 올바르게 동작한다.\n",
    "- 다만 추론이 끝나면 trian()을 선언하여 원래의 훈련 모드로 돌아가게 해주어야 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀분석 예제\n",
    "- 임의로 생성한 텐서들을\n",
    "- 근사하고자 하는 정답 함수에 넣어 정답(y)를 구하고,\n",
    "- 그 정답과 신경망을 통과한 y hat과의 차이를 평균제곱오차를 통해 구하여\n",
    "- 학률적 경사하강법을 통해 최적화 해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3 * x[:,0] + x[:,1] -2 * x[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optim):\n",
    "    # initialize gradients in all parameters in module.\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # feed_forward\n",
    "    y_hat = model(x)\n",
    "\n",
    "    # get error between answer and inferenced.\n",
    "    loss = ((y - y_hat)**2).sum() / x.size(0)\n",
    "\n",
    "    # back-propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # one-step of gradient descent\n",
    "    optim.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "n_epochs = 1000\n",
    "n_iter = 10000\n",
    "\n",
    "model = MyModel(3,1)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.7889e-05) tensor(0.9000) tensor(0.7875)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        x = torch.rand(batch_size, 3)\n",
    "        y = ground_truth(x.data)\n",
    "\n",
    "        loss = train(model, x, y, optim)\n",
    "\n",
    "        avg_loss += loss\n",
    "        avg_loss = avg_loss / n_iter\n",
    "\n",
    "    # simple test ample to check the network\n",
    "    x_valid = torch.FloatTensor([[.3,.2,.1]])\n",
    "    y_valid = ground_truth(x_valid.data)\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "\n",
    "    print(avg_loss, y_valid.data[0], y_hat.data[0,0])\n",
    "\n",
    "    if avg_loss < .001:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn.Module 클래스를 상속받아(forward 함수를 통해) 모델 아키텍처 클래스 선언\n",
    "- 해당 클래스 객체 생성\n",
    "- SGD나 Adam등의 옵티마이저를 생성하고, 생성한 모델의 파라미터를 최적화 대상으로 등록\n",
    "- 데이터로 미니배치를 구성하여 피드포워드 연산 그래프 생성\n",
    "- 손실 함수를 통해 최종 결괏값(scalar)과 손실값(loss) 계산\n",
    "- 3번의 옵티마이저에서 step()을 호출하여 경사하강법 1스텝 수행\n",
    "- 4번으로 돌아가 수렴 조건이 만족할 때까지 반복\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67a89c197df658fa9381c6ce748d39d138b8172be5d6bcdac46c5a692d7ae1fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
