{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서브워드 분절\n",
    "- byte pair encoding 알고리즘을 통한 서브워드 단어 분절은 현재 필수 전처리 방법으로 자리잦ㅂ고 있다.\n",
    "- 서브워드 분절 기법은 기본적으로 '단어는 의미를 가진 더 작은 서브워드들의 조합으로 이루어진다.'는 가정하에 적용되는 알고리즘\n",
    "- 영어나 한국어 역시 라틴어와 한자를 기반으로 형성된 언어이기에 많은 단어가 서브워드들로 구성된다.\n",
    "- 따라서 적절한 서브워드를 발견하여 해당 단위로 쪼개어 주면 어휘 수를 줄일 수 있고 희소성을 효과적으로 줄일 수 있다.\n",
    "- 희소성 감소 외에도, 서브워드 단어 분절로 얻는 가장 대표적인 효과는 UNK토큰에 대한 효율적인 대처이다.\n",
    "- 자연어에서 생성을 포함한 대부분의 딥러닝 자연어 처리 알고리즘은 문장을 입력으로 받을 때 단순히 단어들의 시퀀스로 받아들인다.\n",
    "- 따라서 UNK가 나타나면 이후의 언어 모델의 확률이 크게 망가지고 적절한 문장의 임베딩 또는 생성이 어렵다.\n",
    "- 특히 문장 생성의 경우에는 이전 단어를 기반으로 다음 단어를 예측하므러 더욱 어렵다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 서브워드 단어 분절을 통해 신조어나 오타와 같은 UNK에 대해 서브워드 단위나 문자 단위로 쪼개줌으로서 기존 훈련 데이터에서 보았던 토큰들의 조합을 만들어 버릴 수 있다.\n",
    "- 즉, UNK 자체를 없앰으로써 효율적으로 UNK에 대처할 수 있고, 자연어 처리 알고리즘의 결과물 품질을 향상 시킬 수 있다.\n",
    "- 다만 한글과 영어만으로 훈련한 알고리즘 또는 모델에 아랍어와 같이 전혀 보지 못한 글자가 등장한다면 당연히 UNK로 치환될 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오픈소스\n",
    "- Sennrich(원저자)의 깃허브 : https://github.com/rsennrich/subword_nmt\n",
    "- 본서의 저자가 수정한 버전 : https://github.com/kh-kim/subword-nmt\n",
    "- Google의 SentencePiece 모듈 : https://github.com/google/sentencepiece\n",
    "\n",
    "> 나는 개인적으로 구글의 sentencepiece를 추천한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
